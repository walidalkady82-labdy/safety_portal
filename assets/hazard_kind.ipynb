{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ec092",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_type = {'Unsafe_Condition' :\t0,\n",
    "'Unsafe_Behavior':\t1,\n",
    "'NM':\t2,\n",
    "'FA':\t3\n",
    "}\n",
    "hazard_kind ={ 'Absence(Improper) of safety device.':0,\n",
    "'breaching procedures.':1,\n",
    "'FA - Without PSIF':2,\n",
    "'Failure to communicate':3,\n",
    "'Falling objects':4,\n",
    "'Fire hazard':5,\n",
    "'Improper bonding':6,\n",
    "'Improper Cylinders':7,\n",
    "'Improper design.':8,\n",
    "'Improper Ergonomics.':9,\n",
    "'Improper Tools/Equip.':10,\n",
    "'Improper work environment.':11,\n",
    "'Improper/missing of barricading / guarding':12,\n",
    "'Lack of commitment.':13,\n",
    "'Lack of inspection.':14,\n",
    "'Lack of maintenance.':15,\n",
    "'Lack of resources.':16,\n",
    "'Lighting':17,\n",
    "'LOTOTO':18,\n",
    "'NM - With PSIF':19,\n",
    "'NM - Without PSIF':20,\n",
    "'Not wearing PPE.':21,\n",
    "'Oil Leakage':22,\n",
    "'Poor housekeeping.':23,\n",
    "'Unnecessary scaf.':24,\n",
    "'Unsafe storage':25\n",
    "}\n",
    "Eelect_kind = {\n",
    "    'light': 0 , \n",
    "    'motor' : 1 , \n",
    "    'transformer': 2 , \n",
    "    'generator' : 3 , \n",
    "    'electrical room': 4 , \n",
    "    'earthing' : 5 ,\n",
    "    'Cable' :  6 ,\n",
    "    'panel' :7 ,\n",
    "    'elevator' :8 ,\n",
    "    'sensor' :9, \n",
    "    'hvac' : 10}\n",
    "hazard_level = {'low':0 , 'medium' :1,'high':2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53173aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling data\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "try:\n",
    "    # Attempt to read with potential header issues handled\n",
    "    # The snippet suggests no header, just lines of text.\n",
    "    df = pd.read_csv('electrical_data.csv', header=None, names=['text'], encoding='utf-8', on_bad_lines='skip')\n",
    "except Exception as e:\n",
    "    # Fallback for encoding issues\n",
    "    try:\n",
    "        df = pd.read_csv('electrical_data.csv', header=None, names=['text'], encoding='utf-8-sig', on_bad_lines='skip')\n",
    "    except:\n",
    "        # Last resort fallback\n",
    "        df = pd.read_csv('electrical_data.csv', header=None, names=['text'], encoding='latin1', on_bad_lines='skip')\n",
    "\n",
    "# Define keywords for each label (English and Arabic) based on the user's list and common synonyms\n",
    "keywords = {\n",
    "    0: ['light', 'lamp', 'illumination', 'fixture', 'bulb', 'lux', 'ŸÉÿ¥ÿßŸÅ', 'ÿßŸÜÿßÿ±ÿ©', 'ÿ•ŸÜÿßÿ±ÿ©', 'ÿ•ÿ∂ÿßÿ°ÿ©', 'ÿßÿ∂ÿßÿ°ÿ©', 'ŸÑŸÖÿ®ÿ©', 'ŸÑŸÖÿ®ÿßÿ™', 'ŸÜŸàÿ±'],\n",
    "    1: ['motor', 'engine', 'ŸÖŸàÿ™Ÿàÿ±', 'ŸÖÿßÿ™Ÿàÿ±', 'ŸÖÿ≠ÿ±ŸÉ'],\n",
    "    2: ['transformer', 'ŸÖÿ≠ŸàŸÑ', 'ÿ™ÿ±ÿßŸÜÿ≥'],\n",
    "    3: ['generator', 'ŸÖŸàŸÑÿØ'],\n",
    "    4: ['electrical room', 'electric room','substation', 'switchgear', 'ÿ∫ÿ±ŸÅÿ© ŸÉŸáÿ±ÿ®ÿßÿ°', 'ÿ∫ÿ±ŸÅŸá ŸÉŸáÿ±ÿ®ÿßÿ°', 'ÿ≠ÿ¨ÿ±ÿ© ŸÉŸáÿ±ÿ®ÿßÿ°', 'ŸÖÿ≠ÿ∑ÿ©'],\n",
    "    5: ['earthing', 'earth', 'ground', 'ÿßÿ±ÿ∂Ÿä', 'ÿ£ÿ±ÿ∂Ÿä', 'ÿ™ÿ£ÿ±Ÿäÿ∂', 'ÿßÿ±ÿ≥'],\n",
    "    6: ['cable', 'wire', 'cord', 'wiring', 'ŸÉÿßÿ®ŸÑ', 'ŸÉÿ®ŸÑ', 'ÿ≥ŸÑŸÉ', 'ŸàŸäÿ±', 'ŸàÿµŸÑÿ©', 'ŸàÿµŸÑŸá'],\n",
    "    7: ['panel', 'board', 'box', 'breaker', 'ŸÑŸàÿ≠ÿ©', 'ŸÑŸàÿ≠Ÿá', 'ÿπŸÑÿ®ÿ©', 'ÿπŸÑÿ®Ÿá', 'ŸÇÿßÿ∑ÿπ', 'ÿ™ÿßÿ®ŸÑŸàŸá', 'ŸÉÿ®ŸäŸÜŸá', 'ŸÉÿßÿ®ŸäŸÜÿ©', 'ÿ®Ÿàÿßÿ∑' , 'ÿ≥ŸàŸÉÿ™', 'ÿ®ŸÑÿ¨ÿ©'],\n",
    "    9: ['sensor', 'switch', 'detector', 'ÿ≠ÿ≥ÿßÿ≥', 'ÿ≥ŸäŸÜÿ≥Ÿàÿ±', 'ŸÖŸÅÿ™ÿßÿ≠', 'limit', 'limt', 'valve', 'valv' , 'ÿ®ÿ±ÿ¥ÿ±' , 'ÿ´ÿ±ŸÖŸà ŸÉÿßÿ®ŸÑ']\n",
    "}\n",
    "\n",
    "def get_label(text):\n",
    "    if not isinstance(text, str):\n",
    "        return -1\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    found_labels = []\n",
    "    # Check all keywords\n",
    "    for label, keys in keywords.items():\n",
    "        for k in keys:\n",
    "            if k in text_lower:\n",
    "                found_labels.append(label)\n",
    "                break \n",
    "    \n",
    "    if not found_labels:\n",
    "        return -1\n",
    "    \n",
    "    # Priority Strategy:\n",
    "    # If multiple found, we need a tie-breaker. \n",
    "    # The user provided list is 0-9. \n",
    "    # Common overlaps: \"Motor Cable\" (1 vs 6), \"Panel Switch\" (7 vs 9), \"Substation Room\" (8 vs 4).\n",
    "    # \"Motor Cable\" -> Hazard is usually the cable. Cable (6).\n",
    "    # \"Panel Switch\" -> Hazard is switch. Sensor/Switch (9).\n",
    "    # \"Electrical Room Light\" -> Hazard is Light. Lighting (0).\n",
    "    \n",
    "    # Simple strategy: prioritize component over location, and specific over general.\n",
    "    # 0 (Lighting), 9 (Sensor), 6 (Cable), 7 (Panel), 1 (Motor), 2 (Transformer), 3 (Generator), 5 (Earthing), 4 (Room), 8 (Station).\n",
    "    # Let's try to sort found_labels by this priority? \n",
    "    # Or just use the first found in the loop (which is 0-9).\n",
    "    # Let's stick to 0-9 order for consistency unless obvious. \n",
    "    # Returning the first one found in the 0-9 iteration.\n",
    "    return found_labels[0]\n",
    "\n",
    "# Apply labeling\n",
    "df['label'] = df['text'].apply(get_label)\n",
    "\n",
    "# Save the labeled file\n",
    "df.to_csv('labeled_kinOfHazard.csv', index=False)\n",
    "print(f\"Labeled {len(df)} rows. Distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bdcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating classifier model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP & DATA PREPARATION ---\n",
    "file_path = r'C:\\Users\\walid\\.vscode\\titan_management\\ai\\full_maintenance_training_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "sentences = df['text'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['label'].values)\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "# --- 2. TOKENIZATION ---\n",
    "vocab_size = 3000\n",
    "max_length = 30\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# --- 3. BUILD MODEL ---\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 32, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# --- 4. TRAIN ---\n",
    "print(f\"üöÄ Training Model on {num_classes} categories...\")\n",
    "model.fit(padded, labels, epochs=20, validation_split=0.2, verbose=1)\n",
    "\n",
    "# --- 5. EXPORT: KERAS .H5 (For TFJS) ---\n",
    "print(\"\\nüíæ Saving Keras Model...\")\n",
    "model.save('kind_of_hazard_model.h5')\n",
    "\n",
    "# --- 6. EXPORT: TENSORFLOW LITE (For Mobile) ---\n",
    "print(\"\\nüì± Converting to TensorFlow Lite...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('kind_of_hazard_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# --- 7. EXPORT: METADATA (FIXED) ---\n",
    "print(\"\\nüìù Saving Metadata...\")\n",
    "\n",
    "# Save Tokenizer\n",
    "vocab_path = r'C:\\Users\\walid\\.vscode\\titan_management\\ai\\vocab.json'\n",
    "with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_index, f)\n",
    "\n",
    "# FIX: Convert numpy types to standard python types for JSON\n",
    "# We convert the 'label' to string to be safe\n",
    "mapping = {int(i): str(label) for i, label in enumerate(label_encoder.classes_)}\n",
    "\n",
    "with open('label_mapping.json', 'w') as f:\n",
    "    json.dump(mapping, f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"‚úÖ COMPLETED SUCCESSFULLY!\")\n",
    "print(\"1. 'kind_of_hazard_model.tflite' -> Ready for Flutter\")\n",
    "print(\"2. 'vocab.json' & 'label_mapping.json' -> Ready for logic\")\n",
    "print(\"3. 'kind_of_hazard_model.h5' -> Ready for Web conversion\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f25c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run the conversion\n",
    "#!tensorflowjs_converter --input_format=keras maintenance_model.h5 web_model\n",
    "!tensorflowjs_converter --input_format=keras \"C:\\Users\\walid\\.vscode\\titan_management\\ai\\maintenance_model.h5\" web_model\n",
    "# 3. Zip the result so you can download it\n",
    "!zip -r web_model.zip web_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd59845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi label \n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Load the uploaded text file\n",
    "# Using 'observations.txt'\n",
    "try:\n",
    "    with open('observations.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "except Exception:\n",
    "    # Fallback for encoding\n",
    "    with open('observations.txt', 'r', encoding='latin1') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(lines, columns=['text'])\n",
    "df['text'] = df['text'].str.strip()\n",
    "df = df[df['text'] != ''] # Remove empty lines\n",
    "\n",
    "# Define keywords for each label (English and Arabic)\n",
    "# Based on user request:\n",
    "# 0: Lighting, 1: Motor, 2: Transformer, 3: Generator, 4: Elec Room, \n",
    "# 5: Earthing, 6: Cable, 7: Panel, 8: Substation, 9: Sensor\n",
    "keywords = {\n",
    "    0: ['light', 'lamp', 'illumination', 'fixture', 'bulb', 'lux', 'ŸÉÿ¥ÿßŸÅ', 'ÿßŸÜÿßÿ±ÿ©', 'ÿ•ŸÜÿßÿ±ÿ©', 'ÿ•ÿ∂ÿßÿ°ÿ©', 'ÿßÿ∂ÿßÿ°ÿ©', 'ŸÑŸÖÿ®ÿ©', 'ŸÑŸÖÿ®ÿßÿ™', 'ŸÜŸàÿ±' ,'ŸÉÿ¥ŸÅÿßÿ™' , 'ŸÑŸÖÿ®Ÿá' ,'ÿßÿ∂ÿßÿ°Ÿá','ÿßŸÑÿßÿ∂ÿßÿ°Ÿá' ,'ÿßŸÜÿßÿ±Ÿá'],\n",
    "    1: ['motor', 'engine', 'ŸÖŸàÿ™Ÿàÿ±', 'ŸÖÿßÿ™Ÿàÿ±', 'ŸÖÿ≠ÿ±ŸÉ' , 'ÿ±Ÿàÿ≤ÿ™ÿ©' ,'ŸÉŸÅÿ± ŸÖÿ±Ÿàÿ≠Ÿá'],\n",
    "    2: ['transformer', 'ŸÖÿ≠ŸàŸÑ', 'ÿ™ÿ±ÿßŸÜÿ≥'],\n",
    "    3: ['generator', 'ŸÖŸàŸÑÿØ'],\n",
    "    4: ['electrical room', 'electric room', 'switchgear', 'ÿ∫ÿ±ŸÅÿ© ŸÉŸáÿ±ÿ®ÿßÿ°', 'ÿ∫ÿ±ŸÅŸá ŸÉŸáÿ±ÿ®ÿßÿ°', 'ÿ≠ÿ¨ÿ±ÿ© ŸÉŸáÿ±ÿ®ÿßÿ°', 'substation','ŸÖÿ≠ÿ∑ÿ©'],\n",
    "    5: ['earthing', 'earth', 'ground', 'ÿßÿ±ÿ∂Ÿä', 'ÿ£ÿ±ÿ∂Ÿä', 'ÿ™ÿ£ÿ±Ÿäÿ∂', 'ÿßÿ±ÿ≥'],\n",
    "    6: ['cable', 'wire', 'cord', 'wiring', 'ŸÉÿßÿ®ŸÑ', 'ŸÉÿ®ŸÑ', 'ÿ≥ŸÑŸÉ', 'ŸàŸäÿ±', 'ŸàÿµŸÑÿ©', 'ŸàÿµŸÑŸá'],\n",
    "    7: ['panel', 'board', 'box', 'breaker', 'ŸÑŸàÿ≠ÿ©', 'ŸÑŸàÿ≠Ÿá', 'ÿπŸÑÿ®ÿ©', 'ÿπŸÑÿ®Ÿá', 'ŸÇÿßÿ∑ÿπ', 'ÿ™ÿßÿ®ŸÑŸàŸá', 'ŸÉÿ®ŸäŸÜŸá', 'ŸÉÿßÿ®ŸäŸÜÿ©', 'ÿ®Ÿàÿßÿ∑',  'VCS' , 'ÿ®ÿßŸÜŸÑ'],\n",
    "    8: ['ELevator', 'ÿßÿ≥ÿßŸÜÿ≥Ÿäÿ±', 'ŸÖÿ≠ÿ∑Ÿá'],\n",
    "    9: ['sensor', 'switch', 'detector', 'ÿ≠ÿ≥ÿßÿ≥', 'ÿ≥ŸäŸÜÿ≥Ÿàÿ±', 'ŸÖŸÅÿ™ÿßÿ≠', 'limit', 'limt', 'valve', 'ÿ±Ÿàÿ®ÿ≥Ÿàÿ™ÿ¥', 'ÿ´ÿ±ŸÖŸàŸÖÿ™ÿ±' , 'ŸÑŸäŸÅŸÑ' , 'ÿ®ÿ±ÿ¥ÿ±' , 'ÿ®ÿßŸÑÿ±ÿßÿØÿßÿ±' , 'ÿßŸÑÿ±ÿßÿØÿßÿ±'],\n",
    "    10: ['HVAC' , 'ŸÖÿ±Ÿàÿ≠ÿ©' , 'ÿ™ŸÉŸäŸäŸÅ' ],\n",
    "}\n",
    "\n",
    "def get_label(text):\n",
    "    if not isinstance(text, str):\n",
    "        return -1\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    found_labels = []\n",
    "    # Check all keywords\n",
    "    for label, keys in keywords.items():\n",
    "        for k in keys:\n",
    "            if k in text_lower:\n",
    "                found_labels.append(label)\n",
    "                break \n",
    "    \n",
    "    if not found_labels:\n",
    "        return -1 # Unlabeled\n",
    "    \n",
    "    # Priority Strategy: Return the first one found in the 0-9 iteration order\n",
    "    # This aligns with the user's provided list order.\n",
    "    return found_labels[0]\n",
    "\n",
    "# Apply labeling\n",
    "df['label'] = df['text'].apply(get_label)\n",
    "\n",
    "# Filter out unlabeled rows (optional, but requested \"label list\")\n",
    "# Let's keep them but maybe sort so unlabeled are at the end? \n",
    "# No, let's just output the file.\n",
    "\n",
    "# Save the labeled file\n",
    "df.to_csv('labeled_observations.csv', index=False)\n",
    "print(f\"Labeled {len(df)} rows. Distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc408bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the multi output model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 120\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# --- 2. LOAD & CLEAN DATA ---\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('observations_labeled_data.csv')\n",
    "\n",
    "# 2.1 FIX COLUMN NAMES & TYPOS\n",
    "if 'light' in df.columns: df.rename(columns={'light': 'electrical_kind'}, inplace=True)\n",
    "\n",
    "df['text'] = df['text'].astype(str).str.strip()\n",
    "if 'level' in df.columns:\n",
    "    df['level'] = df['level'].astype(str).str.lower().str.strip()\n",
    "if 'electrical_kind' in df.columns:\n",
    "    df['electrical_kind'] = df['electrical_kind'].astype(str).str.strip()\n",
    "    df.loc[df['electrical_kind'] == 'cable', 'electrical_kind'] = 'Cable'\n",
    "\n",
    "# --- 3. MAPPINGS (REMOVED TYPE) ---\n",
    "HAZARD_KIND_MAPPING = {\n",
    "    'Absence(Improper) of safety device.': 0, 'breaching procedures.': 1, 'FA - Without PSIF': 2,\n",
    "    'Failure to communicate': 3, 'Falling objects': 4, 'Fire hazard': 5, 'Improper bonding': 6,\n",
    "    'Improper Cylinders': 7, 'Improper design.': 8, 'Improper Ergonomics.': 9, 'Improper Tools/Equip.': 10,\n",
    "    'Improper work environment.': 11, 'Improper/missing of barricading / guarding': 12, 'Lack of commitment.': 13,\n",
    "    'Lack of inspection.': 14, 'Lack of maintenance.': 15, 'Lack of resources.': 16, 'Lighting': 17,\n",
    "    'LOTOTO': 18, 'NM - With PSIF': 19, 'NM - Without PSIF': 20, 'Not wearing PPE.': 21,\n",
    "    'Oil Leakage': 22, 'Poor housekeeping.': 23, 'Unnecessary scaf.': 24, 'Unsafe storage': 25\n",
    "}\n",
    "ELEC_KIND_MAPPING = {\n",
    "    'light': 0, 'motor': 1, 'transformer': 2, 'generator': 3, 'electrical room': 4,\n",
    "    'earthing': 5, 'Cable': 6, 'panel': 7, 'elevator': 8, 'sensor': 9, 'hvac': 10\n",
    "}\n",
    "LEVEL_MAPPING = { 'low': 0, 'medium': 1, 'high': 2 }\n",
    "\n",
    "# --- 4. FILL MISSING LABELS ---\n",
    "def get_missing_labels(row):\n",
    "    h = row['hazard_kind'] if row['hazard_kind'] in HAZARD_KIND_MAPPING else 'Lack of maintenance.'\n",
    "    e = row['electrical_kind'] if row['electrical_kind'] in ELEC_KIND_MAPPING else 'panel'\n",
    "    l = row['level'] if row['level'] in LEVEL_MAPPING else 'low'\n",
    "    return pd.Series([h, e, l])\n",
    "\n",
    "df[['hazard_kind', 'electrical_kind', 'level']] = df.apply(get_missing_labels, axis=1)\n",
    "\n",
    "# --- 5. PREPARE INPUTS ---\n",
    "def map_to_int(col, mapping):\n",
    "    return df[col].map(mapping).fillna(0).astype(int).values\n",
    "\n",
    "y_hazard = map_to_int('hazard_kind', HAZARD_KIND_MAPPING)\n",
    "y_elec = map_to_int('electrical_kind', ELEC_KIND_MAPPING)\n",
    "y_level = map_to_int('level', LEVEL_MAPPING)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['text'].astype(str))\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(df['text']), maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# --- 6. CALCULATE SAMPLE WEIGHTS ---\n",
    "def get_weights_dict(y_data):\n",
    "    classes = np.unique(y_data)\n",
    "    weights = class_weight.compute_class_weight('balanced', classes=classes, y=y_data)\n",
    "    return dict(zip(classes, weights))\n",
    "\n",
    "w_hazard = get_weights_dict(y_hazard)\n",
    "w_elec = get_weights_dict(y_elec)\n",
    "w_level = get_weights_dict(y_level)\n",
    "\n",
    "def get_sample_weights(y_data, weight_dict):\n",
    "    return np.array([weight_dict[y] for y in y_data])\n",
    "\n",
    "sw_hazard = get_sample_weights(y_hazard, w_hazard)\n",
    "sw_elec = get_sample_weights(y_elec, w_elec)\n",
    "sw_level = get_sample_weights(y_level, w_level)\n",
    "\n",
    "print(\"Sample weights calculated.\")\n",
    "\n",
    "# --- 7. MODEL DEFINITION (3 OUTPUTS) ---\n",
    "input_text = layers.Input(shape=(MAX_LEN,), name='input_text')\n",
    "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(input_text)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x) \n",
    "\n",
    "# Outputs (Only 3 now)\n",
    "out_hazard = layers.Dense(len(HAZARD_KIND_MAPPING), activation='softmax', name='out_hazard')(x)\n",
    "out_elec = layers.Dense(len(ELEC_KIND_MAPPING), activation='softmax', name='out_elec')(x)\n",
    "out_level = layers.Dense(len(LEVEL_MAPPING), activation='softmax', name='out_level')(x)\n",
    "\n",
    "model = Model(inputs=input_text, outputs=[out_hazard, out_elec, out_level])\n",
    "\n",
    "# --- 8. COMPILE & TRAIN ---\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics={\n",
    "        'out_hazard': 'accuracy',\n",
    "        'out_elec': 'accuracy',\n",
    "        'out_level': 'accuracy'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "model.fit(\n",
    "    X, \n",
    "    [y_hazard, y_elec, y_level], \n",
    "    sample_weight=[sw_hazard, sw_elec, sw_level], \n",
    "    epochs=30,  # Recommended: 30-50 epochs\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 9. EXPORT ---\n",
    "print(\"Converting to TFLite...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('smart_issue_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Save Vocab & Labels\n",
    "with open('vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(tokenizer.word_index, f)\n",
    "\n",
    "def save_labels(mapping, filename):\n",
    "    sorted_labels = [k for k, v in sorted(mapping.items(), key=lambda item: item[1])]\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sorted_labels, f)\n",
    "\n",
    "save_labels(HAZARD_KIND_MAPPING, 'labels_hazard.json')\n",
    "save_labels(ELEC_KIND_MAPPING, 'labels_elec.json')\n",
    "save_labels(LEVEL_MAPPING, 'labels_level.json')\n",
    "\n",
    "print(\"Done! Files generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
